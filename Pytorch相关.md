## 优化器 更新参数的算法 
### Adam:适应性矩估计 adaptive moment estimation
通过计算梯度的一阶矩估计和二阶矩估计而为不同的参数设计独立的自适应性学习率
实现简单，计算高效，对内存需求少;
参数的更新不受梯度的伸缩变换影响;
超参数具有很好的解释性，且通常无需调整或仅需很少的微调;
更新的步长能够被限制在大致的范围内（初始学习率）;
能自然地实现步长退火过程（自动调整学习率）;
很适合应用于大规模的数据及参数的场景;
适用于不稳定目标函数;
适用于梯度稀疏或梯度存在很大噪声的问题

参数有：alpha：同样也称为学习率或步长因子，它控制了权重的更新比率（如 0.001）。
较大的值（如 0.3）在学习率更新前会有更快的初始学习，而较小的值（如 1.0E-5）会令训练收敛到更好的性能。
beta1：一阶矩估计的指数衰减率（如 0.9）。
beta2：二阶矩估计的指数衰减率（如 0.999）。该超参数在稀疏梯度（如在 NLP 或计算机视觉任务中）中应该设置为接近 1 的数。
epsilon：非常小的数，防止在实现中除以零（如 10E-8）。


